{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An M2 VAE for semi supervised modelling on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import keras\n",
    "from keras.layers import Lambda, Input, Dense, LeakyReLU, Concatenate,Dropout,RepeatVector,Reshape,Flatten\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize as norm\n",
    "from collections import Counter\n",
    "from keras import callbacks\n",
    "\n",
    "import random\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of image: i.e. the image will be RESIZE X RESIZE\n",
    "RESIZE = 28\n",
    "\n",
    "# Number of latent z's\n",
    "LATENT_DIM = 300\n",
    "\n",
    "# Number of neurons in each hidden layer\n",
    "INTERMEDIATE_DIM = 1000\n",
    "\n",
    "# Number of classes\n",
    "CLASSES = 10\n",
    "\n",
    "# Number of monte  carlo samples (see paper)\n",
    "MC_SAMPLES = 1\n",
    "\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2():\n",
    "    #Shape of flattened image\n",
    "    input_shape=(RESIZE*RESIZE,)\n",
    "    #inputs\n",
    "    img_input = Input(shape=input_shape)\n",
    "    #labels with empty second half\n",
    "    y_full = Input(shape=(CLASSES,))\n",
    "    #labels with empty labels removed\n",
    "    y_input,y_val = Lambda(lambda x:  tf.split(x,num_or_size_splits=2,axis=0))(y_full)\n",
    "    \n",
    "    def gaussian_sampling(args):\n",
    "        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "        z_mean_repeat = RepeatVector(MC_SAMPLES)(z_mean)\n",
    "        z_log_var_repeat = RepeatVector(MC_SAMPLES)(z_log_var)\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean_repeat))\n",
    "        z_sample = z_mean_repeat + K.exp(0.5 * z_log_var_repeat) * epsilon\n",
    "        return z_sample\n",
    "    \n",
    "    \n",
    "    #Implements a q(y|x) NN with two hidden units that outputs the probability of each img being a certain label\n",
    "    q_y__x_layer1 = Dense(INTERMEDIATE_DIM)(img_input)\n",
    "    q_y__x_layer1_act =LeakyReLU(alpha = .03)(q_y__x_layer1)\n",
    "    q_y__x_layer2 = Dense(INTERMEDIATE_DIM)((q_y__x_layer1_act))\n",
    "    q_y__x_layer2_act = LeakyReLU(alpha = .03)(q_y__x_layer2)\n",
    "    q_y__x_output = Dense(CLASSES, activation = 'softmax',name = 'q_y__x')(q_y__x_layer2_act)\n",
    "    \n",
    "    # Seperates out the predictions that we have labels for and those that we do not\n",
    "    y_sup,y_un= Lambda(lambda x:  tf.split(x,num_or_size_splits=2,axis=0))(q_y__x_output) \n",
    "    # For the integrating out approach, we repeat the input matrix x, and construct a target (bs * n_y) x n_y\n",
    "    # Example of input and target matrix for a 3 class problem and batch_size=2. 2D tensors of the form\n",
    "    #               x_repeat                     t_repeat\n",
    "    #  [[x[0,0], x[0,1], ..., x[0,n_x]]         [[1, 0, 0]\n",
    "    #   [x[1,0], x[1,1], ..., x[1,n_x]]          [1, 0, 0]\n",
    "    #   [x[0,0], x[0,1], ..., x[0,n_x]]          [0, 1, 0]\n",
    "    #   [x[1,0], x[1,1], ..., x[1,n_x]]          [0, 1, 0]\n",
    "    #   [x[0,0], x[0,1], ..., x[0,n_x]]          [0, 0, 1]\n",
    "    #   [x[1,0], x[1,1], ..., x[1,n_x]]]         [0, 0, 1]]\n",
    "    one_hot = Lambda( lambda x: K.constant(np.eye(CLASSES, dtype=int)))(img_input)\n",
    "    #if garbage values change tile to repeat\n",
    "    dummy_y = Lambda( lambda x: K.tile(x, [(BATCH_SIZE//2),1] ))(one_hot)\n",
    "    \n",
    "    y = Concatenate(axis=0)([y_input,dummy_y])\n",
    "    \n",
    "    # turn x,y,z into x,x,x,y,y,y,z,z,z with the number of repeats being the number of classes\n",
    "    img_sup,img_un =Lambda(lambda x:  tf.split(x,num_or_size_splits=2,axis=0))(img_input) \n",
    "    rep_img_un = Lambda(lambda x: K.repeat_elements(x,rep=CLASSES,axis = 0))(img_un)\n",
    "    rep_img_input = Concatenate(axis=0)([img_sup,rep_img_un])\n",
    "    \n",
    "    #Implements a q(z|y,x) NN with two hidden units that outputs the parameters to a gaussian distribution\n",
    "    # for labeled data and for unlabeled outputs parameters for each possible y\n",
    "    q_z__y_x_concat = Concatenate()([rep_img_input,y])\n",
    "    q_z__y_x_layer1 = Dense(INTERMEDIATE_DIM)(q_z__y_x_concat)\n",
    "    q_z__y_x_layer1_act = LeakyReLU(alpha = .03)(q_z__y_x_layer1)\n",
    "    q_z__y_x_layer2 = Dense(INTERMEDIATE_DIM)(q_z__y_x_layer1_act)\n",
    "    q_z__y_x_layer2_act = LeakyReLU(alpha = .03)(q_z__y_x_layer2)\n",
    "    q_z__y_x_mean = Dense(LATENT_DIM,name = 'q_z__y_x_mean')(q_z__y_x_layer2_act)\n",
    "    rep_q_z__y_x_mean = RepeatVector(MC_SAMPLES)(q_z__y_x_mean)\n",
    "    q_z__y_x_log_var = Dense(LATENT_DIM,name = 'q_z__y_x_log_var')(q_z__y_x_layer2_act)\n",
    "    rep_q_z__y_x_log_var = RepeatVector(MC_SAMPLES)(q_z__y_x_log_var)\n",
    "    q_z__y_x_output = Lambda(gaussian_sampling,name = 'q_z__y_x')([q_z__y_x_mean,q_z__y_x_log_var])\n",
    "\n",
    "    # Implements a p(x|y,z) NN with two hidden units that outputs the parameters to a bernoulli distribution\n",
    "    # for labeled data and for unlabeled outputs parameters for each possible y\n",
    "    p_x__y_z_concat = Concatenate()([y ,Flatten()(q_z__y_x_output)])\n",
    "    p_x__y_z_layer1 = Dense(INTERMEDIATE_DIM)(p_x__y_z_concat)\n",
    "    p_x__y_z_layer1_act = LeakyReLU(alpha = .03)(p_x__y_z_layer1)\n",
    "    p_x__y_z_layer2 = Dense(INTERMEDIATE_DIM)(p_x__y_z_layer1_act)\n",
    "    p_x__y_z_layer2_act = LeakyReLU(alpha = .03)(p_x__y_z_layer2)\n",
    "    p_x__y_z_mean = Dense(RESIZE*RESIZE,activation = 'sigmoid',name = 'p_x__y_z_mean')(p_x__y_z_layer2_act)\n",
    "    #p_x__y_z_log_var = Dense(resize*resize,name = 'p_x__a_y_z_log_var')(p_x__a_y_z_layer2_act)\n",
    "    #p_x__y_z_output = Lambda(gaussian_sampling,name = 'p_x__a_y_z')([p_x__a_y_z_mean, p_x__a_y_z_log_var]) \n",
    "    \n",
    "    def gaussian_ll(args):\n",
    "        # Calculates the log liklihood of a point x under a gaussian distribution parameterized by mu and log_var\n",
    "        x , mu, log_var = args\n",
    "        \n",
    "        c = -.5 * math.log(2*math.pi)\n",
    "        density = c - log_var/2 - ((x - mu)/(2*K.exp(log_var) + 1e-8))*(x - mu)\n",
    "\n",
    "        return K.sum(density,axis = -1)\n",
    "    \n",
    "    def unit_gaussian_ll(args):\n",
    "        # Calculates the log liklihood of a point x under a unit gaussian distribution\n",
    "        x = args\n",
    "        \n",
    "        c = -.5 * math.log(2*math.pi)\n",
    "        density = c - (x)**2/2\n",
    "\n",
    "        return K.sum(density,axis = -1)\n",
    "\n",
    "        \n",
    "    def log_pz(y_true,y_pred):\n",
    "        # Calculates the log liklihood that the sampled 'z' is under the unit gaussian distributions \n",
    "        #, then weights the unsupervised samples according to how likely their asscociated y value was.\n",
    "        # (as predicted by p(y|x))\n",
    "        flat_y_un = K.reshape(y_un,shape = [-1])\n",
    "        ones = K.ones(shape = (BATCH_SIZE//2))\n",
    "        weights = K.concatenate([ones,flat_y_un],0)\n",
    "        loss_per_point = weights*K.mean(unit_gaussian_ll(q_z__y_x_output),axis = 1)\n",
    "        split = tf.split(loss_per_point, num_or_size_splits=CLASSES+1 ,axis=0)\n",
    "        sup_loss = split[0]\n",
    "        un = K.concatenate(split[1:])\n",
    "        un_loss = K.sum(K.reshape(un,[BATCH_SIZE//2,CLASSES]),axis = 1)\n",
    "        loss = K.concatenate([sup_loss,un_loss])\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def log_qz(y_true,y_pred):\n",
    "        # Calculates the log liklihood that the sampled 'z' is under the gaussian distributions predicted by\n",
    "        # q(z|y,x), then weights the unsupervised sampled according to how likely their asscociated y value was\n",
    "        #(as predicted by p(y|x))\n",
    "        flat_y_un = K.reshape(y_un,shape = [-1])\n",
    "        ones = K.ones(shape = (BATCH_SIZE//2))\n",
    "        weights = K.concatenate([ones,flat_y_un],0)\n",
    "        loss_per_point = weights*K.mean(gaussian_ll([q_z__y_x_output,rep_q_z__y_x_mean,rep_q_z__y_x_log_var]),axis = 1)\n",
    "        split = tf.split(loss_per_point, num_or_size_splits=CLASSES+1 ,axis=0)\n",
    "        sup_loss = split[0]\n",
    "        un = K.concatenate(split[1:])\n",
    "        un_loss = K.sum(K.reshape(un,[BATCH_SIZE//2,CLASSES]),axis = 1)\n",
    "        loss = K.concatenate([sup_loss,un_loss])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def log_py(y_true,y_pred):\n",
    "        # Calculates the log liklihood that the all possible 'y' is under y's true distribution WHICH\n",
    "        # IS ASSUMED TO BE BALANCED CATAGORICLE, then weights the unsupervised sampled according to \n",
    "        #how likely their asscociated y value was (as predicted by p(y|x)).\n",
    "        flat_y_un = K.reshape(y_un,shape = [-1])\n",
    "        ones = K.ones(shape = (BATCH_SIZE//2))\n",
    "        weights = K.concatenate([ones,flat_y_un],0)\n",
    "        expected = K.ones_like(q_y__x_output)/CLASSES\n",
    "        concat = K.concatenate([y_input,y_un])\n",
    "        loss_per_point = K.categorical_crossentropy(expected,q_y__x_output)\n",
    "        return -loss_per_point\n",
    "        \n",
    "    \n",
    "    def log_px(y_true,y_pred):\n",
    "        # Calculates the log liklihood that the true images is under the gaussian distributions predicted by\n",
    "        # p(x|a,y,z), then weights the unsupervised sampled according to how likely their asscociated y value was\n",
    "        #(as predicted by p(y|a,x))\n",
    "        flat_y_un = K.reshape(y_un,shape = [-1])\n",
    "        ones = K.ones(shape = ((BATCH_SIZE//2)))\n",
    "        weights = K.concatenate([ones,flat_y_un],0)\n",
    "        loss_per_point = -weights*keras.losses.binary_crossentropy(rep_img_input,p_x__y_z_mean)\n",
    "        split = tf.split(loss_per_point, num_or_size_splits=CLASSES+1 ,axis=0)\n",
    "        sup_loss = split[0]\n",
    "        un = K.concatenate(split[1:])\n",
    "        un_loss = K.sum(K.reshape(un,[BATCH_SIZE//2,CLASSES]),axis = 1)\n",
    "        loss = K.concatenate([sup_loss,un_loss])\n",
    "        return loss\n",
    "    \n",
    "    def y_ent(y_true,y_pred):\n",
    "        # Caluclates the entropy of the unsupervised predicted y values\n",
    "        \n",
    "        flat_y_un = K.reshape(y_un,shape = [-1])\n",
    "        zero = K.zeros(shape = ((BATCH_SIZE//2)))\n",
    "        un = flat_y_un*K.log(flat_y_un)\n",
    "        un_loss = K.sum(K.reshape(un,[BATCH_SIZE//2,CLASSES]),axis = 1)\n",
    "\n",
    "        loss = K.concatenate([zero,un_loss])\n",
    "        return -loss\n",
    "       \n",
    "    def acc(y_true,y_pred):\n",
    "        # Calculates the raw accuracy of our y prediction for the images that we have labels for\n",
    "\n",
    "        return K.mean(keras.metrics.categorical_accuracy(y_input,y_sup))\n",
    "    \n",
    "    def y_class(y_true,y_pred):\n",
    "        # Calculates a supervised loss for the y predictions for the images that we have labels for\n",
    "        zero = K.zeros(shape = (BATCH_SIZE//2))\n",
    "        sup_loss = K.categorical_crossentropy(y_input,y_sup)\n",
    "\n",
    "        loss = K.concatenate([sup_loss,zero])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def qy_loss(y_true,y_pred):\n",
    "        return K.mean(1*y_ent(y_true,y_pred) + -1*log_py(y_true,y_pred) + 10*y_class(y_true,y_pred))\n",
    "    \n",
    "    def qz_loss(y_true,y_pred):\n",
    "        return K.mean(log_qz(y_true,y_pred) + -1*log_pz(y_true,y_pred))\n",
    "    \n",
    "    def px_loss(y_true,y_pred):\n",
    "        return K.mean(-log_px(y_true,y_pred))\n",
    "    \n",
    "        \n",
    "    losses = {'q_y__x': qy_loss,'q_z__y_x': qz_loss, 'p_x__y_z_mean':px_loss}\n",
    "    \n",
    "    model = Model([img_input,y_full],[p_x__y_z_mean,q_y__x_output,q_z__y_x_output]\n",
    "                  , name = 'VAE')\n",
    "    model.compile(loss = losses,metrics = {'q_y__x':acc},optimizer = keras.optimizers.Adam(lr=.001,clipnorm=1.,clipvalue= .5))\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (10, 10)             0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               [(None, 784), (None, 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 10), (None,  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (1000, 10)           0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 784)          0           lambda_5[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10)           0           lambda_1[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 794)          0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000)         795000      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1000)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1000)         1001000     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1000)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q_z__y_x_mean (Dense)           (None, 300)          300300      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_z__y_x_log_var (Dense)        (None, 300)          300300      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_z__y_x (Lambda)               (None, 1, 300)       0           q_z__y_x_mean[0][0]              \n",
      "                                                                 q_z__y_x_log_var[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           q_z__y_x[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 310)          0           concatenate_1[0][0]              \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1000)         311000      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         785000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1000)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 1000)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1000)         1001000     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         1001000     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1000)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 1000)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_x__y_z_mean (Dense)           (None, 784)          784784      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_y__x (Dense)                  (None, 10)           10010       leaky_re_lu_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,289,394\n",
      "Trainable params: 6,289,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = M2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the data used for training\n",
    "class TrainGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # Loads in unlabeled images(file paths) and repeats the labeled images until they're\n",
    "    # are more labeled ones then unlabeled ones\n",
    "    def __init__(self,batch_size = 64):\n",
    "        \n",
    "        self.unlabeled_images = read_csv(\"./MNIST/59900_100_balenced/train_x.csv\").to_numpy()\n",
    "        self.labeled_images = read_csv(\"./MNIST/59900_100_balenced/labeled_train_x.csv\").to_numpy()\n",
    "        self.labels = read_csv(\"./MNIST/59900_100_balenced/labeled_train_y.csv\").to_numpy()\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        self.unlabeled_index = np.arange(0, len(self.unlabeled_images), 1).tolist()\n",
    "        random.shuffle(self.labeled_index)\n",
    "        random.shuffle(self.unlabeled_index)\n",
    "        \n",
    "        lis = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        while len(self.labeled_index) <= len(self.unlabeled_index):\n",
    "            random.shuffle(lis)\n",
    "            self.labeled_index.extend(lis)\n",
    "        \n",
    "        self.batch_size = int(batch_size/2)\n",
    "        self.X = np.zeros((self.batch_size*2, RESIZE*RESIZE), dtype='float32')\n",
    "        self.Y = np.zeros((self.batch_size*2,CLASSES), dtype='float32')\n",
    "        \n",
    "    # Number of epochs is number of unlabeled images divided by the batch size\n",
    "    def __len__(self):\n",
    "        return  len(self.labeled_index) // self.batch_size \n",
    "        \n",
    "    # Fetches batch treating image as a matrix of the parameters \n",
    "    # to independent bernoulli distributed random variables, which are\n",
    "    # then sampled from to create a dynamic discretization of the data.\n",
    "    # Also dummy encodes the label.\n",
    "    def __getitem__(self, i):\n",
    "        n = 0\n",
    "        for x in self.labeled_index[i*self.batch_size:(i+1)*self.batch_size]:\n",
    "            \n",
    "            image = self.labeled_images[x] + .5\n",
    "            label = self.labels[x]\n",
    "            rand = np.random.ranf(image.shape)\n",
    "            image = np.greater(image,rand).astype(int)\n",
    "\n",
    "            self.X[n] = image\n",
    "            self.Y[n] = label\n",
    "            n = n + 1\n",
    "            \n",
    "        for x in self.unlabeled_index[i*self.batch_size:(i+1)*self.batch_size]:\n",
    "            \n",
    "            image = self.unlabeled_images[x] + .5\n",
    "            rand = np.random.ranf(image.shape)\n",
    "            image = np.greater(image,rand).astype(int)\n",
    "            self.X[n] = image\n",
    "            n = n + 1\n",
    "\n",
    "        return [self.X , self.Y] , [self.Y,self.Y,self.Y]\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.unlabeled_index)\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        random.shuffle(self.labeled_index)\n",
    "        lis = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        while len(self.labeled_index) <= len(self.unlabeled_index):\n",
    "            random.shuffle(lis)\n",
    "            self.labeled_index.extend(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the data used for validation\n",
    "class ValGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # Loads in the labeled images\n",
    "    def __init__(self,batch_size = 64):\n",
    "        \n",
    "        self.labeled_images = read_csv(\"./MNIST/59900_100_balenced/val_x.csv\").to_numpy()\n",
    "        self.labels = read_csv(\"./MNIST/59900_100_balenced/val_y.csv\").to_numpy()\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.X = np.zeros((self.batch_size*2, RESIZE*RESIZE), dtype='float32')\n",
    "        self.Y = np.zeros((self.batch_size*2,CLASSES), dtype='float32')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return  len(self.labeled_index) // self.batch_size\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        n = 0\n",
    "        for x in self.labeled_index[i*self.batch_size : (i+1)*self.batch_size]:\n",
    "            \n",
    "            image = self.labeled_images[x] + .5\n",
    "            rand = np.random.ranf(image.shape)\n",
    "            image = np.greater(image,rand).astype(int)\n",
    "            label = self.labels[x]\n",
    "\n",
    "            self.X[n] = image\n",
    "            self.Y[n] = label\n",
    "            n = n + 1\n",
    "            \n",
    "        return [self.X, self.Y], [self.Y,self.Y,self.Y]\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_gen = TrainGenerator(BATCH_SIZE)\n",
    "val_gen = ValGenerator(int(BATCH_SIZE/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 5.1909 - p_x__y_z_mean_loss: 0.2461 - q_y__x_loss: 4.8517 - q_z__y_x_loss: 0.0931 - q_y__x_acc: 0.9979 - val_loss: 7.8775 - val_p_x__y_z_mean_loss: 0.2081 - val_q_y__x_loss: 7.6411 - val_q_z__y_x_loss: 0.0283 - val_q_y__x_acc: 0.8402\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 5.0332 - p_x__y_z_mean_loss: 0.2328 - q_y__x_loss: 4.7561 - q_z__y_x_loss: 0.0444 - q_y__x_acc: 1.0000 - val_loss: 7.7690 - val_p_x__y_z_mean_loss: 0.2080 - val_q_y__x_loss: 7.5371 - val_q_z__y_x_loss: 0.0239 - val_q_y__x_acc: 0.8464\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 5.0288 - p_x__y_z_mean_loss: 0.2323 - q_y__x_loss: 4.7489 - q_z__y_x_loss: 0.0476 - q_y__x_acc: 1.0000 - val_loss: 7.9530 - val_p_x__y_z_mean_loss: 0.2099 - val_q_y__x_loss: 7.7182 - val_q_z__y_x_loss: 0.0249 - val_q_y__x_acc: 0.8390\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 5.0285 - p_x__y_z_mean_loss: 0.2321 - q_y__x_loss: 4.7464 - q_z__y_x_loss: 0.0501 - q_y__x_acc: 1.0000 - val_loss: 7.5776 - val_p_x__y_z_mean_loss: 0.2088 - val_q_y__x_loss: 7.3337 - val_q_z__y_x_loss: 0.0350 - val_q_y__x_acc: 0.8628\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 9s 15ms/step - loss: 5.0294 - p_x__y_z_mean_loss: 0.2320 - q_y__x_loss: 4.7444 - q_z__y_x_loss: 0.0530 - q_y__x_acc: 1.0000 - val_loss: 7.9120 - val_p_x__y_z_mean_loss: 0.2144 - val_q_y__x_loss: 7.6614 - val_q_z__y_x_loss: 0.0362 - val_q_y__x_acc: 0.8448\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 5.0312 - p_x__y_z_mean_loss: 0.2318 - q_y__x_loss: 4.7437 - q_z__y_x_loss: 0.0557 - q_y__x_acc: 1.0000 - val_loss: 7.7360 - val_p_x__y_z_mean_loss: 0.2183 - val_q_y__x_loss: 7.4880 - val_q_z__y_x_loss: 0.0297 - val_q_y__x_acc: 0.8509\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 5.0296 - p_x__y_z_mean_loss: 0.2318 - q_y__x_loss: 4.7417 - q_z__y_x_loss: 0.0561 - q_y__x_acc: 1.0000 - val_loss: 7.7783 - val_p_x__y_z_mean_loss: 0.2158 - val_q_y__x_loss: 7.5278 - val_q_z__y_x_loss: 0.0347 - val_q_y__x_acc: 0.8428\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 9s 15ms/step - loss: 4.9933 - p_x__y_z_mean_loss: 0.2315 - q_y__x_loss: 4.7385 - q_z__y_x_loss: 0.0233 - q_y__x_acc: 1.0000 - val_loss: 7.6569 - val_p_x__y_z_mean_loss: 0.2253 - val_q_y__x_loss: 7.4170 - val_q_z__y_x_loss: 0.0146 - val_q_y__x_acc: 0.8537\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 9s 15ms/step - loss: 4.9906 - p_x__y_z_mean_loss: 0.2313 - q_y__x_loss: 4.7372 - q_z__y_x_loss: 0.0220 - q_y__x_acc: 1.0000 - val_loss: 7.5903 - val_p_x__y_z_mean_loss: 0.2237 - val_q_y__x_loss: 7.3542 - val_q_z__y_x_loss: 0.0124 - val_q_y__x_acc: 0.8622\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 9s 15ms/step - loss: 4.9915 - p_x__y_z_mean_loss: 0.2313 - q_y__x_loss: 4.7377 - q_z__y_x_loss: 0.0225 - q_y__x_acc: 1.0000 - val_loss: 7.7254 - val_p_x__y_z_mean_loss: 0.2273 - val_q_y__x_loss: 7.4826 - val_q_z__y_x_loss: 0.0155 - val_q_y__x_acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "# Saves the model best weights to a file \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'NEW_MNIST_M2_VAE.h5', \n",
    "    monitor='val_q_y__x_acc', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    period = 1\n",
    ")\n",
    "\n",
    "# Reduces the learning rate when the model has stoped learning\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',patience = 3 ,factor = .5,verbose = 1)\n",
    "\n",
    "# Trains the model for 10 epochs\n",
    "history = model.fit_generator(\n",
    "    generator = train_gen,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[checkpoint,reduce_lr],\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    epochs=10 ,\n",
    "    max_queue_size = 10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
