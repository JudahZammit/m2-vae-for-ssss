{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple MLP for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import keras\n",
    "from keras.layers import Lambda, Input, Dense, LeakyReLU, Concatenate,Dropout,RepeatVector,Reshape,Flatten\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize as norm\n",
    "from collections import Counter\n",
    "from keras import callbacks\n",
    "\n",
    "import random\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of image: i.e. the image will be RESIZE X RESIZE\n",
    "RESIZE = 28\n",
    "\n",
    "# Number of latent z's\n",
    "LATENT_DIM = 300\n",
    "\n",
    "# Number of neurons in each hidden layer\n",
    "INTERMEDIATE_DIM = 1000\n",
    "\n",
    "# Number of classes\n",
    "CLASSES = 10\n",
    "\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Shape of flattened image\n",
    "input_shape=(RESIZE*RESIZE,)\n",
    "\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "#Implements a q(y|x) NN with two hidden units that outputs the probability of each img being a certain label\n",
    "y = Dense(INTERMEDIATE_DIM)(img_input)\n",
    "y =LeakyReLU(alpha = .03)(y)\n",
    "y = Dense(INTERMEDIATE_DIM)(y)\n",
    "y = LeakyReLU(alpha = .03)(y)\n",
    "y = Dense(CLASSES, activation = 'softmax')(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(img_input,y)\n",
    "model.compile(loss = 'categorical_crossentropy',metrics = ['categorical_accuracy'],optimizer = keras.optimizers.Adam(lr=.001,clipnorm=1.,clipvalue= .5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the data used for training\n",
    "class TrainGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # Loads in unlabeled images(file paths) and repeats the labeled images until they're\n",
    "    # are more labeled ones then unlabeled ones\n",
    "    def __init__(self,batch_size = 64):\n",
    "        \n",
    "        self.unlabeled_images = read_csv(\"./MNIST/59900_100_balenced/train_x.csv\").to_numpy()\n",
    "        self.labeled_images = read_csv(\"./MNIST/59900_100_balenced/labeled_train_x.csv\").to_numpy()\n",
    "        self.labels = read_csv(\"./MNIST/59900_100_balenced/labeled_train_y.csv\").to_numpy()\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        self.unlabeled_index = np.arange(0, len(self.unlabeled_images), 1).tolist()\n",
    "        random.shuffle(self.labeled_index)\n",
    "        random.shuffle(self.unlabeled_index)\n",
    "        \n",
    "        lis = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        while len(self.labeled_index) <= len(self.unlabeled_index):\n",
    "            random.shuffle(lis)\n",
    "            self.labeled_index.extend(lis)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.X = np.zeros((self.batch_size, RESIZE*RESIZE), dtype='float32')\n",
    "        self.Y = np.zeros((self.batch_size,CLASSES), dtype='float32')\n",
    "        \n",
    "    # Number of epochs is number of unlabeled images divided by the batch size\n",
    "    def __len__(self):\n",
    "        return  len(self.labeled_index) // self.batch_size \n",
    "        \n",
    "    # Fetches batch treating image as a matrix of the parameters \n",
    "    # to independent bernoulli distributed random variables, which are\n",
    "    # then sampled from to create a dynamic discretization of the data.\n",
    "    # Also dummy encodes the label.\n",
    "    def __getitem__(self, i):\n",
    "        n = 0\n",
    "        for x in self.labeled_index[i*self.batch_size:(i+1)*self.batch_size]:\n",
    "            \n",
    "            image = self.labeled_images[x] + .5\n",
    "            label = self.labels[x]\n",
    "            rand = np.random.ranf(image.shape)\n",
    "            image = np.greater(image,rand).astype(int)\n",
    "\n",
    "            self.X[n] = image\n",
    "            self.Y[n] = label\n",
    "            n = n + 1\n",
    "\n",
    "        return self.X , self.Y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.unlabeled_index)\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        random.shuffle(self.labeled_index)\n",
    "        lis = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        while len(self.labeled_index) <= len(self.unlabeled_index):\n",
    "            random.shuffle(lis)\n",
    "            self.labeled_index.extend(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the data used for validation\n",
    "class ValGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # Loads in the labeled images\n",
    "    def __init__(self,batch_size = 64):\n",
    "        \n",
    "        self.labeled_images = read_csv(\"./MNIST/59900_100_balenced/val_x.csv\").to_numpy()\n",
    "        self.labels = read_csv(\"./MNIST/59900_100_balenced/val_y.csv\").to_numpy()\n",
    "        \n",
    "        self.labeled_index = np.arange(0, len(self.labeled_images), 1).tolist()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.X = np.zeros((self.batch_size, RESIZE*RESIZE), dtype='float32')\n",
    "        self.Y = np.zeros((self.batch_size,CLASSES), dtype='float32')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return  len(self.labeled_index) // self.batch_size\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        n = 0\n",
    "        for x in self.labeled_index[i*self.batch_size : (i+1)*self.batch_size]:\n",
    "            \n",
    "            image = self.labeled_images[x] + .5\n",
    "            rand = np.random.ranf(image.shape)\n",
    "            image = np.greater(image,rand).astype(int)\n",
    "            label = self.labels[x]\n",
    "\n",
    "            self.X[n] = image\n",
    "            self.Y[n] = label\n",
    "            n = n + 1\n",
    "            \n",
    "        return self.X, self.Y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_gen = TrainGenerator(BATCH_SIZE)\n",
    "val_gen = ValGenerator(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/judah/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.4225 - categorical_accuracy: 0.9378 - val_loss: 0.8659 - val_categorical_accuracy: 0.7280\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.4429 - categorical_accuracy: 0.9311 - val_loss: 0.8478 - val_categorical_accuracy: 0.7365\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.4560 - categorical_accuracy: 0.9269 - val_loss: 0.9310 - val_categorical_accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.4659 - categorical_accuracy: 0.9236 - val_loss: 0.8745 - val_categorical_accuracy: 0.7255\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.3571 - categorical_accuracy: 0.9422 - val_loss: 0.8496 - val_categorical_accuracy: 0.7467\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2954 - categorical_accuracy: 0.9552 - val_loss: 0.8447 - val_categorical_accuracy: 0.7454\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2456 - categorical_accuracy: 0.9639 - val_loss: 0.8587 - val_categorical_accuracy: 0.7389\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2944 - categorical_accuracy: 0.9554 - val_loss: 0.8866 - val_categorical_accuracy: 0.7330\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2630 - categorical_accuracy: 0.9608 - val_loss: 0.8495 - val_categorical_accuracy: 0.7369\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2521 - categorical_accuracy: 0.9628 - val_loss: 0.8607 - val_categorical_accuracy: 0.7411\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
     ]
    }
   ],
   "source": [
    "# Saves the model best weights to a file \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'NEW_MNIST_Baseline.h5', \n",
    "    monitor='val_categorical_accuracy', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    period = 1\n",
    ")\n",
    "\n",
    "# Reduces the learning rate when the model has stoped learning\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',patience = 3 ,factor = .5,verbose = 1)\n",
    "\n",
    "# Trains the model for 10 epochs\n",
    "history = model.fit_generator(\n",
    "    generator = train_gen,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[checkpoint,reduce_lr],\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    epochs=10 ,\n",
    "    max_queue_size = 10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
